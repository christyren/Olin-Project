{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renmengyuan/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from time import time\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from pprint import pformat, pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/data-science-bowl-2019/train.csv' does not exist: b'../input/data-science-bowl-2019/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-efd39de432ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/data-science-bowl-2019/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/data-science-bowl-2019/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/data-science-bowl-2019/train_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/data-science-bowl-2019/specs.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/data-science-bowl-2019/train.csv' does not exist: b'../input/data-science-bowl-2019/train.csv'"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train = pd.read_csv('../input/data-science-bowl-2019/train.csv')\n",
    "test = pd.read_csv('../input/data-science-bowl-2019/test.csv')\n",
    "train_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\n",
    "specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\n",
    "sample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with all the unique 'titles' from the train and test set\n",
    "list_of_user_activities = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n",
    "\n",
    "# make a list with all the unique 'event_code' from the train and test set\n",
    "list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n",
    "list_of_event_id = sorted(list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n",
    "\n",
    "# make a list with all the unique worlds from the train and test set\n",
    "list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n",
    "\n",
    "# create a dictionary numerating the titles\n",
    "activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "assess_titles = sorted(list(set(train[train['type'] == 'Assessment']['title'].unique()).union(set(test[test['type'] == 'Assessment']['title'].unique()))))\n",
    "\n",
    "# replace the text titles with the number titles from the dict\n",
    "train['title'] = train['title'].map(activities_map)\n",
    "test['title'] = test['title'].map(activities_map)\n",
    "train['world'] = train['world'].map(activities_world)\n",
    "test['world'] = test['world'].map(activities_world)\n",
    "train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "\n",
    "# then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "\n",
    "# convert text into datetime\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "test['timestamp'] = pd.to_datetime(test['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(reduce_train['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(reduce_train)\n",
    "    reduce_train['accuracy_group'].hist()\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n",
    "    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True\n",
    "\n",
    "def pretty_json(data):\n",
    "    return json.dumps(json.loads(data), indent=2, sort_keys=False)\n",
    "\n",
    "\n",
    "def pretty_display(df):\n",
    "    style = \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    return display(HTML(df.to_html().replace('\\\\n', '<br>') + style))\n",
    "\n",
    "def get_installation_sample(df):\n",
    "    return df.groupby('installation_id').apply(lambda g: g.sample(random_state=28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that convert the raw data into processed features\n",
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip': 0, 'Activity': 0, 'Assessment': 0, 'Game': 0}\n",
    "    round_max_to_consider = 3\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()}\n",
    "    \n",
    "    round_stats = {}\n",
    "    round_stats['round_time'] = defaultdict(list)\n",
    "    round_stats['round_event_count'] = defaultdict(list)\n",
    "    round_stats['round_miss'] = defaultdict(list)\n",
    "    round_stats['round_count'] = defaultdict(list)\n",
    "    round_stats['incomplete_round'] = defaultdict(list)\n",
    "    round_stats['incomplete_last_round_time'] = []\n",
    "    round_stats['no_round_count'] = 0\n",
    "    \n",
    "    session_stats = {}\n",
    "    session_stats['session_time'] = defaultdict(list)\n",
    "    session_stats['session_event_count'] = defaultdict(list)\n",
    "\n",
    "    # last features\n",
    "    sessions_count = 0\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        installation_id = session['installation_id'].iloc[-1]\n",
    "        \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session) > 1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features['installation_session_count'] = sessions_count\n",
    "            \n",
    "            features.update({'round_time_mean_'+k: np.mean(v) for k, v in round_stats['round_time'].items()})\n",
    "            features.update({'round_time_max_'+k: np.max(v) for k, v in round_stats['round_time'].items()})\n",
    "            features.update({'round_time_min_'+k: np.min(v) for k, v in round_stats['round_time'].items()})\n",
    "            features.update({'round_event_count_mean_'+k: np.mean(v) for k, v in round_stats['round_event_count'].items()})\n",
    "            features.update({'round_event_count_max_'+k: np.max(v) for k, v in round_stats['round_event_count'].items()})\n",
    "            features.update({'round_event_count_min_'+k: np.min(v) for k, v in round_stats['round_event_count'].items()})\n",
    "            features.update({'round_miss_mean_'+k: np.mean(v) for k, v in round_stats['round_miss'].items()})\n",
    "            features.update({'round_miss_max_'+k: np.max(v) for k, v in round_stats['round_miss'].items()})\n",
    "            features.update({'round_miss_min_'+k: np.min(v) for k, v in round_stats['round_miss'].items()})\n",
    "            features.update({'incomplete_round_mean_'+k: np.mean(v) for k, v in round_stats['incomplete_round'].items()})\n",
    "            features.update({'round_count_max_'+k: np.max(v) for k, v in round_stats['round_count'].items()})\n",
    "            features['incomplete_round_count'] = 0\n",
    "            for k, v in round_stats['incomplete_round'].items():\n",
    "                features['incomplete_round_count'] += len(v)\n",
    "            features['incomplete_last_round_time'] = np.mean(round_stats['incomplete_last_round_time'])\n",
    "            features['no_round_count'] = round_stats['no_round_count']\n",
    "            \n",
    "            features.update({'session_time_mean_'+k: np.mean(v) for k, v in session_stats['session_time'].items()})\n",
    "            features.update({'session_time_max_'+k: np.max(v) for k, v in session_stats['session_time'].items()})\n",
    "            features.update({'session_time_min_'+k: np.min(v) for k, v in session_stats['session_time'].items()})\n",
    "            \n",
    "            features.update({'session_event_count_mean_'+k: np.mean(v) for k, v in session_stats['session_event_count'].items()})\n",
    "            features.update({'session_event_count_max_'+k: np.max(v) for k, v in session_stats['session_event_count'].items()})\n",
    "            features.update({'session_event_count_min_'+k: np.min(v) for k, v in session_stats['session_event_count'].items()})\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count),\n",
    "                              ('var_event_id', event_id_count),\n",
    "                               ('var_title', title_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                         \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = installation_id\n",
    "            \n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            \n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            \n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            \n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            \n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features['accuracy'] = accuracy\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            \n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            \n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "                        \n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        if session_type == 'Game':\n",
    "            event_data_dict = {}\n",
    "            completed_event_data = []\n",
    "            current_round = -1\n",
    "            for line in session['event_data'].apply(json.loads):\n",
    "                if 'round' in line and line['round'] not in event_data_dict:\n",
    "                    event_data_dict[line['round']] = [line]\n",
    "                    current_round = line['round']\n",
    "                if current_round > 0:\n",
    "                    event_data_dict[current_round].append(line)\n",
    "                if line['event_code'] == 2030:\n",
    "                    completed_event_data.append(line)\n",
    "\n",
    "            if event_data_dict:\n",
    "                round_stats['round_count'][session_title_text].append(max(event_data_dict.keys()))\n",
    "                for completed in completed_event_data:\n",
    "                    r = completed['round']\n",
    "                    round_stats['round_time'][session_title_text].append((completed['game_time'] - event_data_dict[r][0]['game_time'])/1000)\n",
    "                    round_stats['round_event_count'][session_title_text].append(completed['event_count'] - event_data_dict[r][0]['event_count'])\n",
    "                    if 'misses' in completed:\n",
    "                        round_stats['round_miss'][session_title_text].append(completed['misses'])\n",
    "                completed_rounds = [c['round'] for c in completed_event_data]\n",
    "                for incompleted in set(event_data_dict.keys()) - set(completed_rounds):\n",
    "                    round_stats['incomplete_round'][session_title_text].append(incompleted)\n",
    "                    round_stats['incomplete_last_round_time'].append((event_data_dict[incompleted][-1]['game_time'] - event_data_dict[incompleted][0]['game_time'])/1000)\n",
    "            else:\n",
    "                round_stats['no_round_count'] += 1\n",
    "        \n",
    "        if session_type in ['Game', 'Assessment']:\n",
    "            session_stats['session_time'][session_title_text].append(session.iloc[-1]['game_time']/1000)\n",
    "            session_stats['session_event_count'][session_title_text].append(session.iloc[-1]['event_count'])\n",
    "        \n",
    "        sessions_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type\n",
    "                        \n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments\n",
    "\n",
    "def get_data_test(user_sample):\n",
    "    return get_data(user_sample, True)\n",
    "\n",
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_train_list = map(get_data, [user_sample for _, user_sample in train.groupby('installation_id', sort = False)])\n",
    "    compiled_test_list = list(map(get_data_test, [user_sample for _, user_sample in test.groupby('installation_id', sort = False)]))\n",
    "    for elem in compiled_train_list:\n",
    "        compiled_train += elem\n",
    "        \n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test_list)\n",
    "    categoricals = ['session_title']\n",
    "    return reduce_train, reduce_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = get_train_and_test(train, test)\n",
    "\n",
    "reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\n",
    "reduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]\n",
    "features = [ '2b058fe3', '3393b68b', '363c86c9', '0', '070a5291', '0db6d71d', '12_Monkeys', '1325467d', '2010','37db1c2f', '37ee8496', '392e14df', '3ee399c3', '4022', '4090', '45d01abe', '47026d5f', '4901243f', '499edb7c', '5290eab1', '5348fd84', '562cec5f', '565a3990', '587b5989', '5a848010', '6bf9e3e1', '7372e1a5', '74e5f8a7', '7da34a02', '84538528', '86c924c4', '87d743c1', '884228c8', '8b757ab8', '8fee50e2', '91561152', '9ee1c98c', 'Bug_Measurer__Activity_', 'Crystal_Caves___Level_1', 'Crystal_Caves___Level_2', 'Crystal_Caves___Level_3', 'Ordering_Spheres', 'Rulers', 'Scrub_A_Dub', 'Tree_Top_City___Level_1', 'Tree_Top_City___Level_2', 'Tree_Top_City___Level_3', 'Welcome_to_Lost_Lagoon_', 'a2df0760', 'a5be6304', 'a8876db3', 'a8efe47b', 'acc_Bird_Measurer__Assessment_', 'acc_Cart_Balancer__Assessment_', 'acc_Cauldron_Filler__Assessment_', 'acc_Chest_Sorter__Assessment_', 'acc_Mushroom_Sorter__Assessment_', 'accumulated_accuracy', 'accumulated_accuracy_group', 'accumulated_uncorrect_attempts', 'acf5c23f', 'ad2fc29c', 'b120f2ac', 'b74258a0', 'c51d8688', 'c58186bf', 'c7f7f0e1', 'd185d3ea', 'e4f1efe6', 'e79f3763', 'f54238ee', 'f6947f54', 'f806dc10', 'incomplete_round_count', 'incomplete_round_mean_All_Star_Sorting', 'incomplete_round_mean_Chow_Time', 'incomplete_round_mean_Leaf_Leader', 'incomplete_round_mean_Pan_Balance', 'round_count_max_Air_Show', 'round_count_max_All_Star_Sorting', 'round_count_max_Dino_Dive', 'round_count_max_Dino_Drink', 'round_count_max_Happy_Camel', 'round_count_max_Pan_Balance', 'round_count_max_Scrub_A_Dub', 'round_event_count_max_Air_Show', 'round_event_count_mean_Air_Show', 'round_event_count_mean_All_Star_Sorting', 'round_event_count_mean_Pan_Balance', 'round_event_count_mean_Scrub_A_Dub', 'round_event_count_min_Dino_Dive', 'round_miss_max_All_Star_Sorting', 'round_miss_max_Crystals_Rule', 'round_miss_mean_All_Star_Sorting', 'round_miss_mean_Bubble_Bath', 'round_miss_mean_Crystals_Rule', 'round_miss_mean_Happy_Camel', 'round_miss_mean_Pan_Balance', 'round_miss_mean_Scrub_A_Dub', 'round_miss_min_Chow_Time', 'round_time_max_All_Star_Sorting', 'round_time_mean_All_Star_Sorting', 'round_time_mean_Pan_Balance', 'round_time_min_All_Star_Sorting', 'round_time_min_Pan_Balance', 'session_event_count_max_Bird_Measurer__Assessment_', 'session_event_count_max_Cauldron_Filler__Assessment_', 'session_event_count_max_Mushroom_Sorter__Assessment_', 'session_event_count_mean_Bird_Measurer__Assessment_', 'session_event_count_mean_Bubble_Bath', 'session_event_count_mean_Cauldron_Filler__Assessment_', 'session_event_count_mean_Happy_Camel', 'session_event_count_mean_Mushroom_Sorter__Assessment_', 'session_event_count_min_Cart_Balancer__Assessment_', 'session_event_count_min_Scrub_A_Dub', 'session_time_max_Cauldron_Filler__Assessment_', 'session_time_max_Chest_Sorter__Assessment_', 'session_time_max_Dino_Drink', 'session_time_mean_Bubble_Bath', 'session_time_mean_Scrub_A_Dub', 'session_time_min_Scrub_A_Dub', 'session_title']\n",
    "# print(len(features))\n",
    "cols_to_drop = ['session_id', 'installation_id','accuracy_group',\n",
    "                'installation_session_count',\n",
    "                'installation_duration_mean',\n",
    "                'installation_title_nunique',\n",
    "                'installation_event_code_count_mean',\n",
    "                \"4070\",\n",
    "\n",
    "               ]\n",
    "features = [f for f in features if f not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        self.train_df = train_df.sample(frac=1, random_state=28).reset_index(drop=True)\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = 'accuracy'\n",
    "        self.verbose = verbose\n",
    "        self.params = self.get_params()\n",
    "        self.feat_imp_list = np.zeros(len(features))\n",
    "        self.y_pred, self.score, self.model = self.fit()\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "        oof_pred = np.zeros((len(reduce_train), ))\n",
    "        y_pred = np.zeros((len(reduce_test), ))\n",
    "        val_idx_list = []\n",
    "        cv = GroupKFold(n_splits=self.n_splits)\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(self.train_df, None, self.train_df['installation_id'])):\n",
    "            t, v = self.train_df.iloc[train_idx], self.train_df.iloc[val_idx]\n",
    "            v = get_installation_sample(v)\n",
    "            val_new_idx = [x[1] for x in v.index]\n",
    "            val_idx_list += val_new_idx\n",
    "            \n",
    "            x_train, x_val = t[self.features], v[self.features]\n",
    "            y_train, y_val = t[self.target], v[self.target]\n",
    "            \n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            oof_pred[val_new_idx] = model.predict(conv_x_val).reshape(oof_pred[val_new_idx].shape)\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            print('Partial score of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(v['accuracy_group'], oof_pred[val_new_idx])[1]))\n",
    "        _, loss_score, _ = eval_qwk_lgb_regr(self.train_df['accuracy_group'].iloc[val_idx_list], oof_pred[val_idx_list])\n",
    "        if self.verbose:\n",
    "            print('Our oof cohen kappa score is: ', loss_score)\n",
    "        return y_pred, loss_score, model\n",
    "    \n",
    "class Lgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        self.model = lgb.train(self.params, train_set, valid_sets=[val_set], verbose_eval=verbosity)\n",
    "        self.feat_imp_list += self.model.feature_importance() / self.n_splits\n",
    "        return self.model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'n_estimators':10000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'l2',\n",
    "                    'metric': 'l2',\n",
    "                    'subsample': 0.85,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.008,\n",
    "                    'feature_fraction': 0.85,\n",
    "                    'max_depth': 12,\n",
    "                    'early_stopping_rounds': 200,\n",
    "                    'seed': 866,\n",
    "                    'lambda_l1': 1,  \n",
    "                    'lambda_l2': 1,\n",
    "                 }\n",
    "        return params\n",
    "\n",
    "class Lgb_Model2(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        self.model = lgb.train(self.params, train_set, valid_sets=[val_set], verbose_eval=verbosity)\n",
    "        self.feat_imp_list += self.model.feature_importance() / self.n_splits\n",
    "        return self.model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'n_estimators':10000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.85,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.005,\n",
    "                    'feature_fraction': 0.8,\n",
    "                    'max_depth': 10,\n",
    "                    'early_stopping_rounds': 200,\n",
    "                    'seed': 886,\n",
    "                    'lambda_l1': 1,  \n",
    "                    'lambda_l2': 1,\n",
    "                 }\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lgb_model = Lgb_Model(reduce_train, reduce_test, features, categoricals=categoricals)\n",
    "lgb_model2 = Lgb_Model2(reduce_train, reduce_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_pred = lgb_model.y_pred*0.5 + lgb_model2.y_pred*0.5\n",
    "\n",
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "reduce_train['accuracy_group'].hist()\n",
    "\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(final_pred, acum * 100)\n",
    "print(bound)\n",
    "\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "final_pred = np.array(list(map(classify, final_pred)))\n",
    "\n",
    "sample_submission['accuracy_group'] = final_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
